# StarPU --- Runtime system for heterogeneous multicore architectures.
#
# Copyright (C) 2020       UniversitÃ© de Bordeaux, CNRS (LaBRI UMR 5800), Inria
#
# StarPU is free software; you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or (at
# your option) any later version.
#
# StarPU is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#
# See the GNU Lesser General Public License in COPYING.LGPL for more details.
#
from starpu import starpupy
import starpu
import joblib
import asyncio
import math
import functools

# get the number of CPUs controlled by StarPU
def cpu_count():
	n_cpus=starpupy.cpu_worker_get_count()
	return n_cpus

# split a list ls into n_block numbers of sub-lists 
def partition(ls, n_block):
	if len(ls)>=n_block:
		# there are n1 sub-lists which contain q1 elements, and (n_block-n1) sublists which contain q2 elements (n1 can be 0)
		q1=math.ceil(len(ls)/n_block)
		q2=math.floor(len(ls)/n_block)
		n1=len(ls)%n_block
		#n2=n_block-n1
		# generate n1 sub-lists in L1, and (n_block-n1) sub-lists in L2
		L1=[ls[i:i+q1] for i in range(0, n1*q1, q1)]
		L2=[ls[i:i+q2] for i in range(n1*q1, len(ls), q2)]

		L=L1+L2
	else:
		# if the block number is larger than the length of list, each element in the list is a sub-list
		L=[ls[i:i+1] for i in range (len(ls))]
	return L

def future_generator(g, n_jobs, dict_task):
	# g is generated by delayed function, after converting to a list, the format is [function, (arg1, arg2, ... ,)]
	L=list(g)
	# generate a list of function according to g
	def lf(ls):
		L_func=[]
		for i in range(len(ls)):
			# the first element is the function
			f=ls[i][0]
			# the second element is the args list of a type tuple
			L_args=list(ls[i][1])
			# generate a list of function
			L_func.append(f(*L_args))
		return L_func
	# get the number of block
	if n_jobs<-cpu_count()-1 or n_jobs>cpu_count():
		print("Error: n_jobs is out of range, number of CPUs is", cpu_count())
	elif n_jobs<0:
		n_block=cpu_count()+1+n_jobs
	else:
		n_block=n_jobs
	# generate the split function list
	L_split=partition(L,n_block)
	# operation in each split list
	L_fut=[]
	for i in range(len(L_split)):
		fut=starpu.task_submit(name=dict_task['name'], synchronous=dict_task['synchronous'], priority=dict_task['priority'],\
							   color=dict_task['color'], flops=dict_task['flops'], perfmodel=dict_task['perfmodel'])\
			                  (lf, L_split[i])
		L_fut.append(fut)
	return L_fut

class Parallel(joblib.Parallel):
	def __init__(self, mode="normal", perfmodel=None, end_msg=None,\
			 name=None, synchronous=0, priority=0, color=None, flops=None,\
	         n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs',\
	         batch_size='auto', temp_folder=None, max_nbytes='1M',\
	         mmap_mode='r', prefer=None, require=None):
		super(Parallel, self).__init__(n_jobs=None, backend=None, verbose=0, timeout=None, pre_dispatch='2 * n_jobs',\
	         batch_size='auto', temp_folder=None, max_nbytes='1M',\
	         mmap_mode='r', prefer=None, require=None)

		self.mode=mode
		self.perfmodel=perfmodel
		self.end_msg=end_msg
		self.name=name
		self.synchronous=synchronous
		self.priority=priority
		self.color=color
		self.flops=flops
		self.n_jobs=n_jobs

	def __call__(self,iterable):
		#generate the dictionary of task_submit
		dict_task={'name': self.name, 'synchronous': self.synchronous, 'priority': self.priority, 'color': self.color, 'flops': self.flops, 'perfmodel': self.perfmodel}
		# the mode normal, user can call the function directly without using async
		if self.mode=="normal":
			#def parallel_normal(g):
			async def asy_main():
				L_fut=future_generator(iterable, self.n_jobs, dict_task)
				res=[]
				for i in range(len(L_fut)):
					L_res=await L_fut[i]
					res.extend(L_res)
				#print(res)
				return res
			asyncio.run(asy_main())
			return asy_main
			#return parallel_normal
		# the mode future, user needs to use asyncio module and await the Future result in main function
		elif self.mode=="future":
			#def parallel_future(g):
			L_fut=future_generator(iterable, self.n_jobs, dict_task)
			fut=asyncio.gather(*L_fut)
			if self.end_msg==None:
				return fut
			else:
				fut.add_done_callback(functools.partial(print, self.end_msg))
				return fut
			#return fut
			#return parallel_future

def delayed(f):
	def delayed_func(*args):
		return f, args
	return delayed_func


######################################################################
class Memory(joblib.Memory):
	def __init__(self,location=None, backend='local', cachedir=None,
                 mmap_mode=None, compress=False, verbose=1, bytes_limit=None,
                 backend_options=None):
		super(Memory, self).__init__(location=None, backend='local', cachedir=None,
                 mmap_mode=None, compress=False, verbose=1, bytes_limit=None,
                 backend_options=None)


def dump(value, filename, compress=0, protocol=None, cache_size=None):
	return joblib.dump(value, filename, compress, protocol, cache_size)

def load(filename, mmap_mode=None):
	return joblib.load(filename, mmap_mode)

def hash(obj, hash_name='md5', coerce_mmap=False):
	return joblib.hash(obj, hash_name, coerce_mmap)

def register_compressor(compressor_name, compressor, force=False):
	return joblib.register_compressor(compressor_name, compressor, force)

def effective_n_jobs(n_jobs=-1):
	return joblib.effective_n_jobs(n_jobs)

class parallel_backend(joblib.parallel_backend):
	def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None,
                 **backend_params):
		super(parallel_backend, self).__init__(backend, n_jobs=-1, inner_max_num_threads=None,
                 **backend_params)

def register_parallel_backend(name, factory, make_default=False):
	return joblib.register_parallel_backend(name, factory, make_default)