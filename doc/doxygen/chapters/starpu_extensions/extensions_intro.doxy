/* StarPU --- Runtime system for heterogeneous multicore architectures.
 *
 * Copyright (C) 2009-2022  Universit√© de Bordeaux, CNRS (LaBRI UMR 5800), Inria
 *
 * StarPU is free software; you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License as published by
 * the Free Software Foundation; either version 2.1 of the License, or (at
 * your option) any later version.
 *
 * StarPU is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 *
 * See the GNU Lesser General Public License in COPYING.LGPL for more details.
 */

/*! \intropage{IntroExtensions, -- StarPU Extensions --}

In this part, we further present the knowledge of StarPU. For users who need StarPU not only for task submission but for some more complex applications, you should read this part according to your needs.

<ul>
<li> We provides several tools to help debugging applications in \ref DebuggingTools.

<li> You can learn more knowledge about some important and core concepts in StarPU:
    <ul>
    <li> After reading \ref TasksInStarPU, you can get more information about how to manage tasks in StarPU in section: \ref AdvancedTasksInStarPU.
    <li> After reading \ref DataManagement, you can know more about how to manage the data layout of your applications in section: \ref AdvancedDataManagement.
    <li> After reading \ref Scheduling, You can get some advanced scheduling policies in StarPU in section: \ref AdvancedScheduling, and also \ref SchedulingContexts, \ref SchedulingContextHypervisor.
    </ul>

<li> You can also learn some further usages of StarPU in the rest of this part:
    <ul>
    <li> If you need to store more data than what the main memory (RAM) can store, the section \ref OutOfCore presents how to add a new memory node on a disk and how to use it.
    <li> We integrate MPI transfers within task parallelism. For users who need to run MPI processes in their applications the section \ref MPISupport may be useful.
    <li> In section \ref TCPIPSupport, we explain the TCP/IP master slave mechanism which can execute application across many remote cores without thinking about data distribution.
    <li> The section \ref Transactions can help you cancel a sequence of already submitted tasks based on a just-in-time decision.
    <li> StarPU provides some supports for failure of tasks or even failure of complete nodes in \ref FaultTolerance.
    <li> The usage of <c>libstarpufft</c> is described in \ref FFTSupport whose design is very similar to both <c>fftw</c> and <c>cufft</c>, but this library provided by StarPU takes benefit from both CPUs and GPUs.
    <li> StarPU support Field Programmable Gate Array (FPGA) applications exploiting DFE configurations, you can find related usage in section \ref MaxFPGASupport.
    <li> If you want your applications can share entities such as Events, Contexts or Command Queues between several OpenCL implementations, we have an OpenCL implementation based on StarPU described in \ref SOCLOpenclExtensions.
    <li> We propose a hierarchical tasks model in the section \ref HierarchicalDAGS to enable tasks subgraphs at runtime for a more dynamic task graph.
    <li> You can find how to partition a machine into clusters with the cluster API which depends on the <c>hwloc</c> tool in the section \ref ClusteringAMachine.
    <li> If you need StarPU to coexist with other parallel software elements without resulting in computing core oversubscription or undersubscription, the section \ref InteroperabilitySupport is useful. You can get the information about how to dynamically manage the computing resources allocated to StarPU.
    <li> You can learn how to define a StarPU task scheduling policy in a basic monolithic way, or in a modular way in the section \ref HowToDefineANewSchedulingPolicy.
    <li> The section \ref SimGridSupport show you how to simulate execution on an arbitrary platform.
    </ul>
</ul>

*/
